{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/train.csv').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_columns = dataset.columns[1:]\n",
    "original_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,)\n",
      "(31915,)\n",
      "(127656, 6)\n",
      "(31915, 6)\n"
     ]
    }
   ],
   "source": [
    "X, y = dataset.comment_text, dataset[original_columns]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "for s in X_train, X_test, y_train, y_test:\n",
    "    print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_columns = original_columns[:]\n",
    "our_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize to get a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vectorizer here\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=word_tokenize, max_df=0.5, max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 252 ms, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### peek at most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''      195335\n",
       "of      179591\n",
       "you     172937\n",
       "is      144467\n",
       "that    128526\n",
       "``      124835\n",
       "it      118395\n",
       "in      116092\n",
       "!        86829\n",
       "for      82139\n",
       "this     77983\n",
       "not      77438\n",
       ")        72695\n",
       "on       71852\n",
       "(        68374\n",
       "be       66634\n",
       ":        66611\n",
       "as       62085\n",
       "have     59286\n",
       "are      58744\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vect_train.sum(axis=0).T, index=vectorizer.get_feature_names())[0]\\\n",
    "    .sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 20000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit a classifier for each output column and make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6: toxic\n",
      "let's fit a classifier... done!\n",
      "let's predict... done! score: 0.954\n",
      "2/6: severe_toxic\n",
      "let's fit a classifier... done!\n",
      "let's predict... done! score: 0.986\n",
      "3/6: obscene\n",
      "let's fit a classifier... done!\n",
      "let's predict... done! score: 0.976\n",
      "4/6: threat\n",
      "let's fit a classifier... done!\n",
      "let's predict... done! score: 0.997\n",
      "5/6: insult\n",
      "let's fit a classifier... "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions_simple = []\n",
    "scores = []\n",
    "classifiers = []\n",
    "for n, col in enumerate(our_columns):\n",
    "    print('{}/{}: {}'.format(n + 1, len(our_columns), col))\n",
    "    \n",
    "    print('let\\'s fit a classifier', end='... ')\n",
    "    # define a classifier here\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(64, 32, 16))\n",
    "    classifier.fit(vect_train, y_train[col])\n",
    "    classifiers.append(classifier)\n",
    "    print('done!')\n",
    "\n",
    "    print('let\\'s predict', end='... ')\n",
    "    vect_test = vectorizer.transform(X_test)\n",
    "    y_pred = pd.Series(classifier.predict(vect_test), y_test.index)\n",
    "    predictions_simple.append(y_pred)\n",
    "    \n",
    "    score = (y_pred == y_test[col]).mean()\n",
    "    scores.append(score)\n",
    "    print('done! score: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predf_simple = pd.concat(predictions_simple, axis=1)\n",
    "predf_simple.columns = original_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def mean_log_loss(predf):\n",
    "    return np.mean([log_loss(real, predicted) for ((_, predicted), (_, real)) in zip(predf.items(), y_test.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_log_loss(predf_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map 0s and 1s to global cutoff values\n",
    "cutoff = 0.04\n",
    "predf_cut_004 = predf_simple.applymap(lambda x: min(1 - cutoff, max(cutoff, x)))\n",
    "mean_log_loss(predf_cut_004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (super ugly) map 0s and 1s to the columnwise cutoff values\n",
    "cutoffs = [1 - score for score in scores]\n",
    "predf_score_cut = pd.concat([s.apply(lambda x: min(1 - cutoff, max(cutoff, x))) for ((_, s), cutoff) in zip(predf_simple.items(), cutoffs)], axis=1)\n",
    "mean_log_loss(predf_score_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save partial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_set = pd.read_csv('data/test.csv').set_index('id')\n",
    "competition_set.head()\n",
    "X_competition = competition_set.comment_text\n",
    "\n",
    "competition_predictions = []\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    print('here we are :) {}'.format(original_columns[i]))\n",
    "    vect_competition = vectorizer.transform(X_competition)\n",
    "    y_competition = pd.Series(classifier.predict(vect_competition), X_competition.index)\n",
    "    competition_predictions.append(y_competition)\n",
    "    \n",
    "competition_df = pd.concat(competition_predictions, axis=1)\n",
    "competition_df.columns = original_columns\n",
    "\n",
    "competition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [1 - score for score in scores]\n",
    "competition_df_score_cut = pd.concat([s.apply(lambda x: min(1 - cutoff, max(cutoff, x))) \n",
    "                                      for ((_, s), cutoff)\n",
    "                                      in zip(competition_df.items(), cutoffs)], axis=1)\n",
    "competition_df_score_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_df_score_cut.to_csv('attempt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('attempt4.pickle', 'wb'):\n",
    "    pickle.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:toxic]",
   "language": "python",
   "name": "conda-env-toxic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
